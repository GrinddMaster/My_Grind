{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOW4NfEOyhLJ4EgjgwVV7kF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrinddMaster/My_Grind/blob/main/GpuRuntime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqRsRy7fFxNr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "metadata": {
        "id": "lrRRBjGPF-3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/retinal-disease-classification/Labels.csv')\n",
        "print(data.head())\n",
        "images_path = '/content/drive/MyDrive/Colab Notebooks/retinal-disease-classification/Crs_val'\n",
        "\n",
        "print(data.shape)\n",
        "disease_counts = data.iloc[:, 2:].sum().sort_values(ascending=False)\n",
        "fig = px.bar(disease_counts, title=\"Disease Distribution in Training Set\")\n",
        "fig.show()\n",
        "\n",
        "threads = 10"
      ],
      "metadata": {
        "id": "0IC1k8apGBbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(img_id):#this function returns all the images and lables in one big array\n",
        "  images = []\n",
        "  for path in img_id:\n",
        "    img_path = os.path.join(images_path, str(path)+\".png\")\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is not None:\n",
        "      image = cv2.resize(img, (246, 246))\n",
        "      images.append(image)\n",
        "    else:\n",
        "        print(f\"Failed to load image from path: {path}\")\n",
        "  return np.array(images)\n",
        "def chunks_divide(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "data_chunk_size = len(data) // threads\n",
        "data_chunks_list = list(chunks_divide(data[\"ID\"], data_chunk_size))\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=threads) as executor:\n",
        "    image_chunks = executor.map(load_images, data_chunks_list)\n",
        "\n",
        "Xtrain = np.concatenate(list(image_chunks))\n",
        "#Now all the iamges and Lables are in Xtrain and Ytrain."
      ],
      "metadata": {
        "id": "S2OgndDaGDxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_lables(img_id):\n",
        "  labels = []\n",
        "  for label in img_id:\n",
        "    row = data[data['ID'] == label].iloc[0]\n",
        "    labels.append(row['Disease_Risk'])\n",
        "  return np.array(labels)\n",
        "\n",
        "def chunks_divide(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "label_chunk_size = len(data) // threads\n",
        "label_chunks_list = list(chunks_divide(data[\"ID\"], label_chunk_size))\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=threads) as executor:\n",
        "    label_chunks = executor.map(load_lables, label_chunks_list)\n",
        "\n",
        "Ytrain = np.concatenate(list(label_chunks))\n",
        "print(Ytrain.shape)"
      ],
      "metadata": {
        "id": "I384GRcNGIR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain = Xtrain/255.0 #Normalize the data."
      ],
      "metadata": {
        "id": "rCZ1kPvUGLgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Model_Training():\n",
        "  model = Sequential([\n",
        "      Conv2D(32, (3, 3), activation='relu', input_shape=(246, 246, 3)),\n",
        "      MaxPooling2D((2, 2)),\n",
        "      Conv2D(64, (3, 3), activation='relu'),\n",
        "      MaxPooling2D((2, 2)),\n",
        "      Conv2D(128, (3, 3), activation='relu'),\n",
        "      MaxPooling2D((2, 2)),\n",
        "      Flatten(),\n",
        "      Dense(46, activation='relu'),\n",
        "      Dense(50, activation='relu'),\n",
        "      Dense(182, activation='relu'),\n",
        "      Dense(40, activation='relu'),\n",
        "      Dropout(0.5),\n",
        "      Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "jR78HGKFGP35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model_Training()\n",
        "\n",
        "kf = KFold(n_splits=20)\n",
        "fold_no = 1\n",
        "X = Xtrain.copy()\n",
        "Y = Ytrain.copy()\n",
        "for train_index, val_index in kf.split(X):\n",
        "    Xtrain_fold,Xval_fold = X[train_index],X[val_index]\n",
        "    Ytrain_fold,Yval_fold = Y[train_index],Y[val_index]\n",
        "    print(f'Training fold {fold_no}...')\n",
        "\n",
        "    history=model.fit(Xtrain_fold, Ytrain_fold, epochs=10, validation_data=(Xval_fold, Yval_fold))\n",
        "\n",
        "    fold_no += 1\n"
      ],
      "metadata": {
        "id": "rO6F5TOVGSSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "accuracy = history_dict['accuracy']\n",
        "val_accuracy = history_dict['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "#\n",
        "# Plot the model accuracy vs Epochs\n",
        "#\n",
        "ax[0].plot(epochs, accuracy, 'r', label='Training accuracy')\n",
        "ax[0].plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "ax[0].set_title('Training & Validation Accuracy', fontsize=16)\n",
        "ax[0].set_xlabel('Epochs', fontsize=16)\n",
        "ax[0].set_ylabel('Accuracy', fontsize=16)\n",
        "ax[0].legend()\n",
        "#\n",
        "# Plot the loss vs Epochs\n",
        "#\n",
        "ax[1].plot(epochs, loss_values, 'r', label='Training loss')\n",
        "ax[1].plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "ax[1].set_title('Training & Validation Loss', fontsize=16)\n",
        "ax[1].set_xlabel('Epochs', fontsize=16)\n",
        "ax[1].set_ylabel('Loss', fontsize=16)\n",
        "ax[1].legend()"
      ],
      "metadata": {
        "id": "6uZ0BBREGVmE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}